"use client"

// import {File} from "@web-stb/file"
import AllowToUseMicrophone from "@/app/(auxiliary)/components/Blocks/FormBlock/CoupleOfInputs/CurrentInput/Message/VoiceInput/AllowToUseMicrophone";
import ReadyVoice from "@/app/(auxiliary)/components/Blocks/FormBlock/CoupleOfInputs/CurrentInput/Message/VoiceInput/ReadyVoice";
import Button from "@/app/(auxiliary)/components/UI/Button/Button";
import Microphone from "@/app/(auxiliary)/components/UI/SVG/Microphone/Microphone";
import { formattedTime } from "@/app/(auxiliary)/func/formattedTime";
import { useAppDispatch, useAppSelector } from "@/app/(auxiliary)/libs/redux-toolkit/store/hooks";
import {
    selectFormFileData,
    selectRejectionInputs,
    selectServerResponse
} from "@/app/(auxiliary)/libs/redux-toolkit/store/slices/UserFormDataSlice/UserFormDataSlice";
import { MESSAGE_KEY, MessageInputDataType } from "@/app/(auxiliary)/types/AppTypes/InputHooksTypes";
import { MessageType } from "@/app/(auxiliary)/types/Data/Interface/RootPage/RootPageContentType";
import { blue_light, red_dark } from "@/styles/colors";
import { FC, useEffect, useLayoutEffect, useRef, useState } from "react";
import styles from "./AudioPlayer.module.scss";
import { setNewNotification } from "@/app/(auxiliary)/libs/redux-toolkit/store/slices/AppSlice/AppSlice";
import { inputsNameError } from "../../inputsNameError";
import { boldSpanTag } from "@/app/(auxiliary)/func/tags/boldSpanTag";
import { span } from "framer-motion/client";

interface PropsType {
    voicePlaceholder?: MessageType;
    setNewMessage: (newMessage: File, validationStatus: boolean) => void;
    removeRecorder: () => void;
    isError: boolean;
    changeMessageTypeHandler: (newType: MessageInputDataType) => void;
    setErrorHandler: (status: boolean) => void;

}

const VoiceInput: FC<PropsType> = ({
    voicePlaceholder,
    setNewMessage,
    isError,
    setErrorHandler,
    changeMessageTypeHandler,
    removeRecorder
}) => {
    const dispatch = useAppDispatch()
    const voiceMessage = useAppSelector(selectFormFileData)[MESSAGE_KEY]
    const serverResponse = useAppSelector(selectServerResponse).status
    const rejectionInputs = useAppSelector(selectRejectionInputs)

    const [isRecording, setIsRecording] = useState(false)
    const [recordingIsDone, setRecordingIsDone] = useState<boolean>(voiceMessage.value instanceof File)
    const [microphonePermission, setMicrophonePermission] = useState<PermissionState>()
    const [noMicrophone, setNoMicrophone] = useState<boolean>(false)
    const [modalToAllowUseMicro, setModalToAllowUseMicro] = useState<boolean>(false)

    const [audioBlob, setAudioBlob] = useState<Blob | null>(() => {
        if (voiceMessage.value instanceof File) {
            return new Blob(
                [voiceMessage.value],
                { type: voiceMessage.value.type }
            )
        }

        return null
    })

    const mediaRecorderRef = useRef<MediaRecorder | null>(null)
    const audioChunksRef = useRef<Blob[]>([])
    const recordingButtonRef = useRef<HTMLButtonElement | null>(null)

    const startRecording = async () => {
        if (noMicrophone) {
            changeMessageTypeHandler("text")
            dispatch(setNewNotification({
                message: `–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –º—ã –Ω–µ —Å–º–æ–≥–ª–∏ –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –≤–∞—à–µ–º—É –º–∏–∫—Ä–æ—Ñ–æ–Ω—É üòû<br/>${boldSpanTag("–û–ø–∏—à–∏—Ç–µ –≤–∞—à—É –ø—Ä–æ–±–ª–µ–º—É –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –±–ª–æ–∫–µ")} üìù`,
                "type": "warning"
            }))
            return
        }

        if (microphonePermission === "denied") {
            dispatch(setNewNotification({ message: `–í—ã –∑–∞–ø—Ä–µ—Ç–∏–ª–∏ –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –¥–ª—è –Ω–∞—à–µ–≥–æ —Å–∞–π—Ç–∞ üòû<br/> –ù–æ –µ—Å–ª–∏ –í—ã –ø–µ—Ä–µ–¥—É–º–∞–ª–∏, ${boldSpanTag("–Ω–∞–∂–º–∏—Ç–µ –Ω–∞ –∑–Ω–∞—á–æ–∫ –∑–∞–º–∫–∞ üîí –∏–ª–∏ –∑–Ω–∞—á–æ–∫ –Ω–∞—Å—Ç—Ä–æ–µ–∫ ‚öôÔ∏è —Ä—è–¥–æ–º —Å –∞–¥—Ä–µ—Å–æ–º —Å–∞–π—Ç–∞")} –≤ —Å—Ç—Ä–æ–∫–µ –±—Ä–∞—É–∑–µ—Ä–∞.<br/> –¢–∞–º –≤—ã —Å–º–æ–∂–µ—Ç–µ ${boldSpanTag("—Ä–∞–∑—Ä–µ—à–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞")}.`, type: "warning" }))
            return
        }

        setIsRecording((prevState) => !prevState)
        audioChunksRef.current = []

        setModalToAllowUseMicro(microphonePermission === "prompt")

        /**
         * –î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
         */
        const stream = await navigator.mediaDevices.getUserMedia({
            audio: true,
            video: false
        })

        /**
         * –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∞–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–Ω–æ–ø–∫–∏ –∑–∞–ø–∏—Å–∏ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–∞ –≥—Ä–æ–º–∫–æ—Å—Ç—å –∑–≤—É–∫–∞
         * –°–æ–∑–¥–∞–Ω–∏–µ –∞—É–¥–∏–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ –∞—É–¥–∏–æ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
         */
        const audioContext = new AudioContext()
        const source = audioContext.createMediaStreamSource(stream)
        const analyser = audioContext.createAnalyser() // –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–≤—É–∫–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
        let recordingStatus = true

        /**
         * –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–∞—Å—Ç–æ—Ç–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –º–µ–∂–¥—É –≤—ã–∑–æ–≤–∞–º–∏ –º–µ—Ç–æ–¥–∞ getByteFrequencyData(). –ó–Ω–∞—á–µ–Ω–∏–µ –±–ª–∏–∂–µ –∫ 1 –¥–∞—Å—Ç –ø–ª–∞–≤–Ω—ã–π –ø–µ—Ä–µ—Ö–æ–¥, –∞ –∫ 0 - —Ä–µ–∑–∫–∏–π.
         */
        analyser.smoothingTimeConstant = .3
        /**
         * –ü–∞—Ä–∞–º–µ—Ç—Ä –∑–∞–¥–∞–µ—Ç —Ä–∞–∑–º–µ—Ä Fast Fourier Transform (FFT), –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —á–∞—Å—Ç–æ—Ç–Ω–æ–π —Å–æ—Å—Ç–∞–≤–ª—è—é—â–µ–π —Å–∏–≥–Ω–∞–ª–∞. –ë–æ–ª—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–∞—Å—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ —á–∞—Å—Ç–æ—Ç–∞—Ö.
         */
        analyser.fftSize = 1024
        const dataArray = new Uint8Array(analyser.frequencyBinCount)
        source.connect(analyser)
        // source.disconnect()

        const lerp = (start: number, end: number, t: number) => start + t * (end - start)
        let currentPosition = 100

        const analyzeVolume = () => {
            analyser.getByteFrequencyData(dataArray)

            const sum = dataArray.reduce((a, b) => a + b, 0)
            const averageVolume = (sum / dataArray.length) * 2 // –°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å –≥—Ä–æ–º–∫–æ—Å—Ç–∏

            // –ò–∑–º–µ–Ω—è–µ–º —Ñ–æ–Ω –∫–Ω–æ–ø–∫–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≥—Ä–æ–º–∫–æ—Å—Ç–∏ (–ø—Ä–∏–º–µ—Ä —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–º)
            const intensity = Math.min(Math.max(averageVolume / 255, 0), 1) // –ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –æ—Ç 0 –¥–æ 1

            // const gradientPosition = lerp(currentPosition, 100 - intensity * 100, 0.1) // –ü—Ä–æ—Ü–µ–Ω—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞
            // currentPosition = gradientPosition

            // const gradientColor1 = `rgba(${lerp(38, 255, intensity)}, ${lerp(167, 50, intensity)}, ${lerp(227, 50, intensity)}, 1)`
            // const gradientColor2 = `rgba(${lerp(255, 255, 1 - intensity)}, ${lerp(255, 255, 1 - intensity)}, ${lerp(255, 255, 1 - intensity)}, 1)`

            // if (recordingButtonRef.current) {
            //     recordingButtonRef.current.style.background = `radial-gradient(circle, ${gradientColor1} 0%, ${gradientColor2} ${gradientPosition}%)`
            // }

            const gradientPosition = 100 - (intensity * 100) // –ü—Ä–æ—Ü–µ–Ω—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞

            if (recordingButtonRef.current) {
                // recordingButtonRef.current.style.transform = `scale(${newScale})`
                recordingButtonRef.current.style.background = `radial-gradient(circle, rgba(38,167,227,1) 0%, rgba(255,255,255, 1) ${gradientPosition}%)`
            }

            if (recordingStatus) {
                requestAnimationFrame(analyzeVolume)
            }
        }
        requestAnimationFrame(analyzeVolume)

        mediaRecorderRef.current = new MediaRecorder(stream)

        mediaRecorderRef.current.ondataavailable = (event) => {
            audioChunksRef.current.push(event.data)
        }

        mediaRecorderRef.current.onstop = () => {
            const blob = new Blob(audioChunksRef.current, { type: "audio/mp3" })

            if (blob) {
                setAudioBlob(blob)

                const newAudioFile = new File(
                    [blob],
                    `voice-record-${formattedTime()}`,
                    {
                        type: "audio/wav",
                        lastModified: Date.now()
                    }
                )
                setNewMessage(newAudioFile, true)
            }

            /**
             * –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–æ—Ä–æ–∂–µ–∫ –∏, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
             */
            stream.getTracks().forEach(track => track.stop());
            recordingStatus = false
            audioContext.close()
        };

        mediaRecorderRef.current.start();
    }

    const stopRecording = () => {
        if (mediaRecorderRef.current) {
            mediaRecorderRef.current.stop()
        }

        setIsRecording((prevState) => !prevState)
        setRecordingIsDone((prevState) => !prevState)
    }

    const deleteCurrentRecord = () => {
        removeRecorder()
        setRecordingIsDone(false)
        setAudioBlob(null)
    }

    useEffect(() => {
        if (serverResponse === "success") {
            setAudioBlob(null)
            setRecordingIsDone(false)
        }
    }, [
        serverResponse
    ]);

    useEffect(() => {
        if (!isError && rejectionInputs.length) {
            if (rejectionInputs.includes("message")) {
                dispatch(setNewNotification({
                    message: `${boldSpanTag(inputsNameError[MESSAGE_KEY])}: –°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º`,
                    type: "error"
                }))
                setErrorHandler(true)
            }
        }
    }, [rejectionInputs, isError, dispatch, setErrorHandler])

    useLayoutEffect(() => {
        /**
         * Checking, if user's devices has microphone and access to use it
         */
        setNoMicrophone(typeof navigator.mediaDevices?.getUserMedia !== "function")

        if (navigator.permissions) {
            navigator.permissions.query({ name: "microphone" as PermissionName }).then((permissions) => {
                // The permissions.state can be 'granted', 'denied', or 'prompt'
                // You can control the permission to use user's microphone
                setMicrophonePermission(permissions.state)

                permissions.onchange = () => {
                    setMicrophonePermission(permissions.state)

                    setModalToAllowUseMicro(false)
                }
            }).catch(() => {
                console.warn("Microphone permission query is not supported in this browser.")
            })
        } else {
            console.warn("Permissions API is not available in this browser")
        }
    }, []);

    useEffect(() => {
        if (modalToAllowUseMicro) {
            const timer = setTimeout(() => {
                setModalToAllowUseMicro(false)
            }, 3000)

            return () => {
                clearTimeout(timer)
            }
        }
    }, [modalToAllowUseMicro])

    return (
        <>
            <div className={styles.voiceInputWrapper}>
                {(audioBlob && recordingIsDone) ? (
                    <ReadyVoice audioBlob={audioBlob}
                        removeCurrentRecord={deleteCurrentRecord} />
                ) : (
                    <Button onClick={isRecording ? () => stopRecording() : () => startRecording()}
                        buttonRef={recordingButtonRef}
                        className={isRecording ? styles.recordingButton : ""}
                        style={{ backgroundColor: isError ? red_dark : blue_light }}
                        image={{
                            position: "left",
                            children: <Microphone />,
                            visibleOnlyImage: false
                        }}>
                        {isRecording ? voicePlaceholder?.recordingPlaceholder : voicePlaceholder?.inputPlaceholder}
                    </Button>
                )}
            </div>

            {modalToAllowUseMicro ? (
                <AllowToUseMicrophone isRecording={isRecording}
                    microphonePermission={microphonePermission}
                    stopRecording={stopRecording} />
            ) : null}
        </>
    )
};

export default VoiceInput;